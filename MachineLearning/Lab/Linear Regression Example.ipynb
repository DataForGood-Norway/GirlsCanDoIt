{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Linear Regression using GapMinder Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a very simple example of machine learning we're going see if we can predict life expectancy using some data from [GapMinder](https://www.gapminder.org/tools/#_data_/_lastModified:1526038652718&lastModified:1526038652718;&chart-type=bubbles), and organisation that aims to educate us more on the true state of the world.\n",
    "The input data set will have the following variables:\n",
    "- Country - 136 countries are included \n",
    "- Continent\n",
    "- Life expectancy\n",
    "- GDP per capita, PPP, inflation adjusted health_spend\n",
    "- Healthcare spend as a percentage of GDP\n",
    "- Popultaion per square km \n",
    "- The democracy index of the country (high is better). See https://en.wikipedia.org/wiki/Democracy_Index\n",
    "\n",
    "Go to [GapMinder](https://www.gapminder.org/tools/#_data_/_lastModified:1526038652718&lastModified:1526038652718;&chart-type=bubbles) now and take a look at some of these variable in the data viewer.  Do any of the variables (visually) look like they have a bearing on life expectancy?  Are any of them surprising?\n",
    "<img src=\"files/Screen Shot from gapminder.png\" alt=\"GDP per Capita vs. Life Expectancy from Gapminder\" title=\"GDP per Capita vs. Life Expectancy from Gapminder\" style=\"width: 50pc;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Linear Regression Theory\n",
    "- Straight line equation (indicate dependent vs. independent variable)\n",
    "- Least squares\n",
    "- Expand to multiple dimensions\n",
    "- Link to more detailed explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we know which variables to use?\n",
    "## Backward elimination\n",
    "1. Choose a maximum P value: 5% is a good value\n",
    "2. Run the linear regression using all the dependent variables. \n",
    "3. Look at the P values from the output, and choose the biggest.  \n",
    "4. If it is > than 5%, then drop that variable.\n",
    "5. Rerun the linear regression.\n",
    "6. Repeat steps 2-5 until all P-values are < 5%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding\n",
    "## Importing the Dataset\n",
    "Firstly we need to import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard library for numerical analysis\n",
    "import numpy as np\n",
    "# Standard library for data manipulation\n",
    "import pandas as pd\n",
    "# User-defined library for making plots\n",
    "import sys\n",
    "sys.path.insert(0, './python')\n",
    "from plot_functions import make_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./python', '', '/Users/harjeet/DevEnv/miniconda3/lib/python36.zip', '/Users/harjeet/DevEnv/miniconda3/lib/python3.6', '/Users/harjeet/DevEnv/miniconda3/lib/python3.6/lib-dynload', '/Users/harjeet/.local/lib/python3.6/site-packages', '/Users/harjeet/DevEnv/miniconda3/lib/python3.6/site-packages', '/Users/harjeet/DevEnv/miniconda3/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', '/Users/harjeet/DevEnv/miniconda3/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg', '/Users/harjeet/DevEnv/miniconda3/lib/python3.6/site-packages/IPython/extensions', '/Users/harjeet/.ipython']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import the dataset with data for 2007, which has the following columns:\n",
    "Country - 136 countries are included\n",
    "Continent\n",
    "lifeexp - Life expectancy\n",
    "gdpPercap - GDP per capita, PPP, inflation adjusted\n",
    "health_spend - Healthcare spend as a percentage of GDP\n",
    "Pop density - popultaion per square km\n",
    "Democracy - The democracy index of the country (high is better).  See https://en.wikipedia.org/wiki/Democracy_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/gapminder_2007_emma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is how the data looks:\n",
    "\n",
    "<img src=\"files/Screen Shot dataset.png\" alt=\"Data_table\" title=\"Data table\" style=\"width: 50pc;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Data preparation\n",
    "To get ready for the regression we need to make sure that all the columns contain numbers only:\n",
    "- Cells that are empty currently contain \"Not a Number\" (nan) - these will be replaced with an average\n",
    "- Cells that contain words (\"categorical variables\") will be replaced with numerical values.\n",
    "\n",
    "But firstly the data now needs to be split into independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country, continent, population, GDP per cap, healthcare spend, pop density, democracy\n",
    "X = dataset.iloc[:,[0,1,3,4,5,6,7]].values\n",
    "# Will use the log of the GDP per capita because from our first look, there seems to be a log-linear realtionship\n",
    "X[:,3] = np.log(dataset.iloc[:,4].values)\n",
    "# The dependent variable is life expectancy, in column 2\n",
    "y = dataset.iloc[:, 2].values\n",
    "# Also making a list of continents for plotting purposes\n",
    "cont = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "- The cells where data is missing have \"Not a Number\" in them. The following code replaces these with the average for that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the \"NaNs\" - replace with the average of that column\n",
    "# sklearn contains libraries for preprocessing data\n",
    "# now importing Imputer class\n",
    "from sklearn.preprocessing import Imputer\n",
    "# Create object\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "# Fit imputer object to feature X\n",
    "imputer = imputer.fit(X[:,2:])\n",
    "# Replace missing data with replaced values\n",
    "X[:,2:] = imputer.transform(X[:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "Categorical variables are ones which have a name, not a number.  In this case that would \"Country name\" and \"Continent\".\n",
    "\n",
    "__Country name__: We only have each country listed once, and it makes no sense to use the name of the country in the regression, so we are somply going to number them 0-136\n",
    "\n",
    "__Continent__: There are 5 continents in this dataset: Africa, Americas, Asia, Europe and Oceania.  If we simply give them a number 0-4, Python will assume that Oceania \"is greater\" than all this other continents, which makes no sense.  To fix this we use \"one-hot encoding\" which works like this:\n",
    "- Number the continents 0-4\n",
    "- Create a column for each continent -- these new columns are called \"dummy variables\"\n",
    "- If the country is in that continent the column contains \"1\" otherwise it contains \"0\"\n",
    "- Finally, since any of the five columns could be predicted from the other four (known as the \"dummy variable trap\") we need to drop one.\n",
    "    - Normally it would be the first column, but since Africa looks quite interesting in this dataset, I decided to drop Oceania\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harjeet/DevEnv/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nColumns are now\\n0: constant, 1:is Africa 2: is Americas 3: is Asia 4:is Europe\\n5: country 6:population 7:log(GDP per cap) \\n8:health spend 9:pop density 10: dempocracy score\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now importing LabelEncoder and OneHotEncoder classes\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Encoding the Independent Variable\n",
    "# Firstly assign the categorical variables a unique number using LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "# Country names are in column zero\n",
    "# Each country will get a number 0-136 (for the full dataset) \n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "# Each continent will get a number 0-4\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "cont = labelencoder_X.fit_transform(cont)\n",
    "\n",
    "# one hot encode continenets\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# Remove a continent (dummy variable trap)\n",
    "# The obvious one to drop is Africa as it's the first column\n",
    "# However this is also the most interesting\n",
    "# so dropping Oceania instead\n",
    "X = X[:,[0,1,2,3,5,6,7,8,9,10]]\n",
    "\n",
    "# Add a column of ones to the begining.  \n",
    "# This is because the following procedure \n",
    "# will otherwise not have a constant in the regression model\n",
    "X = np.append(arr = np.ones((len(X),1)).astype(int), values = X, axis = 1)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test, cont_train, cont_test = train_test_split(X, y, cont, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\"\"\"\n",
    "Columns are now\n",
    "0: constant, 1:is Africa 2: is Americas 3: is Asia 4:is Europe\n",
    "5: country 6:population 7:log(GDP per cap) \n",
    "8:health spend 9:pop density 10: dempocracy score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Start the training\n",
    "\"\"\"\n",
    "\n",
    "#train the regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "#Make predictions\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "#plot the results\n",
    "gdp_per_cap = np.exp(X_train[:,7])\n",
    "make_plot('GDP per cap',gdp_per_cap,y_train,y_pred,cont_train)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_train[:,8],y_train,y_pred,cont_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   28.69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 31 May 2018</td> <th>  Prob (F-statistic):</th> <td>9.53e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:05:04</td>     <th>  Log-Likelihood:    </th> <td> -351.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   108</td>      <th>  AIC:               </th> <td>   724.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   754.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   36.5923</td> <td>    9.214</td> <td>    3.972</td> <td> 0.000</td> <td>   18.306</td> <td>   54.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  -15.0632</td> <td>    5.374</td> <td>   -2.803</td> <td> 0.006</td> <td>  -25.728</td> <td>   -4.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -2.0509</td> <td>    5.012</td> <td>   -0.409</td> <td> 0.683</td> <td>  -11.998</td> <td>    7.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -4.9746</td> <td>    5.329</td> <td>   -0.934</td> <td> 0.353</td> <td>  -15.551</td> <td>    5.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -2.0982</td> <td>    4.883</td> <td>   -0.430</td> <td> 0.668</td> <td>  -11.790</td> <td>    7.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0094</td> <td>    0.016</td> <td>    0.584</td> <td> 0.561</td> <td>   -0.023</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>  1.27e-09</td> <td> 4.07e-09</td> <td>    0.312</td> <td> 0.756</td> <td>-6.81e-09</td> <td> 9.35e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    4.4054</td> <td>    0.715</td> <td>    6.161</td> <td> 0.000</td> <td>    2.986</td> <td>    5.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.1395</td> <td>    0.300</td> <td>   -0.465</td> <td> 0.643</td> <td>   -0.735</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0004</td> <td>    0.001</td> <td>    0.409</td> <td> 0.684</td> <td>   -0.002</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0470</td> <td>    0.139</td> <td>   -0.337</td> <td> 0.736</td> <td>   -0.323</td> <td>    0.229</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.983</td> <th>  Durbin-Watson:     </th> <td>   1.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.083</td> <th>  Jarque-Bera (JB):  </th> <td>   5.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.230</td> <th>  Prob(JB):          </th> <td>  0.0572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.029</td> <th>  Cond. No.          </th> <td>3.40e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.747\n",
       "Model:                            OLS   Adj. R-squared:                  0.721\n",
       "Method:                 Least Squares   F-statistic:                     28.69\n",
       "Date:                Thu, 31 May 2018   Prob (F-statistic):           9.53e-25\n",
       "Time:                        13:05:04   Log-Likelihood:                -351.40\n",
       "No. Observations:                 108   AIC:                             724.8\n",
       "Df Residuals:                      97   BIC:                             754.3\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.5923      9.214      3.972      0.000      18.306      54.879\n",
       "x1           -15.0632      5.374     -2.803      0.006     -25.728      -4.398\n",
       "x2            -2.0509      5.012     -0.409      0.683     -11.998       7.896\n",
       "x3            -4.9746      5.329     -0.934      0.353     -15.551       5.602\n",
       "x4            -2.0982      4.883     -0.430      0.668     -11.790       7.593\n",
       "x5             0.0094      0.016      0.584      0.561      -0.023       0.042\n",
       "x6           1.27e-09   4.07e-09      0.312      0.756   -6.81e-09    9.35e-09\n",
       "x7             4.4054      0.715      6.161      0.000       2.986       5.825\n",
       "x8            -0.1395      0.300     -0.465      0.643      -0.735       0.455\n",
       "x9             0.0004      0.001      0.409      0.684      -0.002       0.003\n",
       "x10           -0.0470      0.139     -0.337      0.736      -0.323       0.229\n",
       "==============================================================================\n",
       "Omnibus:                        4.983   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.083   Jarque-Bera (JB):                5.722\n",
       "Skew:                          -0.230   Prob(JB):                       0.0572\n",
       "Kurtosis:                       4.029   Cond. No.                     3.40e+09\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.4e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start backward elimination\n",
    "import statsmodels.formula.api as sm\n",
    "X_opt = X_train[ : , [0,1,2,3,4,5,6,7,8,9,10]]\n",
    "regressor_OLS = sm.OLS(endog = y_train, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
