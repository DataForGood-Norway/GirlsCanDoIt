{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Linear Regression using GapMinder Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a very simple example of machine learning, we're going see if we can predict life expectancy using some data from [GapMinder](https://www.gapminder.org/tools/#_data_/_lastModified:1526038652718&lastModified:1526038652718;&chart-type=bubbles), an organisation that aims to educate us more on the true state of the world.\n",
    "The input data set will have 136 countries with the following variables:\n",
    "- Continent\n",
    "- Life expectancy\n",
    "- GDP per capita, PPP, inflation adjusted\n",
    "- Healthcare spend as a percentage of GDP\n",
    "- Population per square km \n",
    "- The democracy index of the country (high is better). See https://en.wikipedia.org/wiki/Democracy_Index\n",
    "\n",
    "Go to [GapMinder](https://www.gapminder.org/tools/#$state$time$value=2007;;&data$_lastModified:1526038652718&lastModified:1526038652718;&chart-type=bubbles) now and take a look at some of these variable in the data viewer.  \n",
    "- Do any of the variables (visually) look like they have a bearing on life expectancy?  \n",
    "- Are any of them surprising?\n",
    "\n",
    "#### Question: is there a country that is particularly inefficient in terms of healthcare spend?\n",
    "<img src=\"https://github.com/DataForGood-Norway/GirlsCanDoIt/blob/master/MachineLearning/Lab/files/Screen%20Shot%20from%20gapminder.png?raw=true\" alt=\"GDP per Capita vs. Life Expectancy from Gapminder\" title=\"GDP per Capita vs. Life Expectancy from Gapminder\" style=\"width: 50pc;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we know which variables to use?\n",
    "When we have a number of variables, it may not be immediately apparent which ones influence the final result.  We will usually find that we can and should  drop one or more independent variables because:\n",
    "- We don't want to include variables that aren't significant\n",
    "<img src=\"https://github.com/DataForGood-Norway/GirlsCanDoIt/blob/master/MachineLearning/Lab/files/GarbageInGarbageOut.png?raw=true\" alt=\"Garbage In - Garbage out\" title=\"Garbage In - Garbage Out\" style=\"width: 25pc;\"/>\n",
    "- When you are presenting your results you will have to explain why you included a variable; \"because I had the data\" is not a good enough reason!\n",
    "\n",
    "There are [several ways to acheive this](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) but we will concentrate on \"backward elimination\".  To understand this you need to know about the \"P\" value.  A proper explanation is [here](https://www.mathbootcamps.com/what-is-a-p-value) but for now we'll just say that the lower the P value is, the more significant the variable.\n",
    "\n",
    "## Backward elimination\n",
    "1. Choose a maximum P value: 5% is a good value\n",
    "2. Run the linear regression using all the dependent variables. \n",
    "3. Look at the P values from the output, and choose the biggest.  \n",
    "4. If it is > than 5%, then drop that variable.\n",
    "5. Rerun the linear regression.\n",
    "6. Repeat steps 2-5 until all P-values are < 5%.  \n",
    "\n",
    "#### I will take you up to step 5, then the rest will be up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding\n",
    "Firstly we need to import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard library for numerical analysis\n",
    "import numpy as np\n",
    "# Standard library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# User-defined library for making plots\n",
    "import sys\n",
    "!mkdir -p local_modules/\n",
    "!cd local_modules\n",
    "!wget https://raw.githubusercontent.com/DataForGood-Norway/GirlsCanDoIt/master/MachineLearning/Lab/python/plot_functions.py\n",
    "!cd ..\n",
    "sys.path.append('local_modules')\n",
    "from plot_functions import make_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset\n",
    "\n",
    "Next we will import the dataset with data for 2007, which has the following columns:\n",
    "- Country - 136 countries are included\n",
    "- Continent\n",
    "- lifeexp - Life expectancy\n",
    "- gdpPercap - GDP per capita, PPP, inflation adjusted\n",
    "- health_spend - Healthcare spend as a percentage of GDP\n",
    "- Pop density - popultaion per square km\n",
    "- Democracy - The democracy index of the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://raw.githubusercontent.com/DataForGood-Norway/GirlsCanDoIt/master/MachineLearning/Lab/datasets/gapminder_2007_emma.csv')\n",
    "dataset.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Firstly the data now needs to be split into independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country, continent, population, GDP per cap, healthcare spend\n",
    "# pop density, democracy\n",
    "# So we keep all lines (':') but get rid off the 'lifeExp' column\n",
    "X_df = dataset.loc[:, dataset.columns != 'lifeExp']\n",
    "# Will use the log of the GDP per capita because from our first look\n",
    "# there seems to be a log-linear realtionship\n",
    "# The log function is applied to every element x of the column 'gdpPercap'\n",
    "X_df['logGDP'] = X_df['gdpPercap'].apply(lambda x: np.log(x))\n",
    "# Then we can get rid of 'gdpPercap'\n",
    "X_df = X_df.drop('gdpPercap', 1)\n",
    "\n",
    "# The dependent variable is life expectancy, labelled 'lifeExp'\n",
    "y_df = dataset['lifeExp']\n",
    "# Also making a list of continents for plotting purposes\n",
    "cont = dataset.loc[:,'continent'].tolist()\n",
    "\n",
    "X_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "To get ready for the regression we need to make sure that all the columns contain numbers only:\n",
    "- Cells that are empty currently contain \"Not a Number\" (nan) - these will be replaced with an average\n",
    "- Cells that contain words (\"categorical variables\") will be replaced with numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the \"NaNs\" - replace with the average of that column\n",
    "# sklearn contains libraries for preprocessing data\n",
    "# now importing Imputer class\n",
    "from sklearn.preprocessing import Imputer\n",
    "# Create object\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "# Fit imputer object to feature X to every column with numerical values\n",
    "imputer = imputer.fit(X_df.loc[:,['pop', 'health_spend', 'Pop density', 'Democracy', 'logGDP']])\n",
    "# Replace missing data with replaced values\n",
    "X_df.loc[:,['pop', 'health_spend', 'Pop density', 'Democracy', 'logGDP']] = imputer.transform(X_df.loc[:,['pop', 'health_spend', 'Pop density', 'Democracy', 'logGDP']])\n",
    "X_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "Categorical variables are ones which have a name, not a number.  In this case that would \"Country name\" and \"Continent\".\n",
    "\n",
    "### Country name\n",
    "We only have each country listed once, and it makes no sense to use the name of the country in the regression, so we are simply going to number them 0-136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now importing LabelEncoder class\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Encoding the Independent Variable\n",
    "# Firstly assign the categorical variables a unique number using LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "# Country names are in column zero\n",
    "# Each country will get a number 0-136 (for the full dataset) \n",
    "X_df[['country']] = labelencoder_X.fit_transform(X_df.loc[:,'country'])\n",
    "X_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continent\n",
    "There are 5 continents in this dataset: Africa, Americas, Asia, Europe and Oceania.  If we simply give them a number 0-4, Python will assume that Oceania \"is greater\" than all this other continents, which makes no sense.  To fix this we use \"one-hot encoding\" which works like this:\n",
    "- Number the continents 0-4\n",
    "- Create a column for each continent -- these new columns are called __dummy variables__\n",
    "- If the country is in that continent the column contains \"1\" otherwise it contains \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now importing OneHotEncoder class\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cont = labelencoder_X.fit_transform(X_df.loc[:,'continent'])\n",
    "# one hot encode continents\n",
    "\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "X_df = pd.concat([X_df,pd.get_dummies(X_df['continent'], prefix='continent')], axis=1)\n",
    "\n",
    "# now drop the original 'continentG' column (you don't need it anymore)\n",
    "X_df.drop(['continent'],axis=1, inplace=True)\n",
    "X_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variable trap\n",
    "A requirement of regression is that no column can be predicted from the others. Since any of the five columns could be predicted from the other four (known as the __dummy variable trap__) we need to drop one.\n",
    "- Normally it would be the first column, but since Africa looks quite interesting in this dataset, I decided to drop Oceania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.drop(['continent_Oceania'], axis=1, inplace=True)\n",
    "X_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a constant\n",
    "We need to add a column of ones at the beginning, as the regression requires a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.append(arr = np.ones((len(X),1)).astype(int), values = X, axis = 1)\n",
    "X_df['constant_for_regression'] = 1\n",
    "X_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into tesing and training set\n",
    "This is very important!  We will first try to create the model from the training set and then apply it to the test set to see if the model has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, cont_train, cont_test = train_test_split(X_df, y_df, cont, test_size = 0.2, random_state = 0)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the regressor\n",
    "This is where we get to the interesting part!\n",
    "\n",
    "Firstly we will train the regressor using all the variables and see how it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Start the training\n",
    "\"\"\"\n",
    "\n",
    "#train the regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "#Make predictions\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "#plot the results\n",
    "make_plot('GDP per cap',np.exp(X_train['logGDP']),y_train,y_pred,cont_train)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_train['health_spend'],y_train,y_pred,cont_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What can we say about the above plots?  \n",
    "#### Question: Is there a continent that stands out from the rest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start backward elimination\n",
    "As a reminder:\n",
    "1. Choose a maximum P value: 5% is a good value\n",
    "2. Run the linear regression using all the dependent variables.\n",
    "3. Look at the P values from the output, and choose the biggest.\n",
    "4. If it is > than 5%, then drop that variable.\n",
    "5. Rerun the linear regression.\n",
    "6. Repeat steps 2-5 until all P-values are < 5%.\n",
    "\n",
    "#### The code below is the first step and then you will have a chance to get involved and complete the other steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start backward elimination\n",
    "X_opt = X_train.loc[:,['constant_for_regression', 'continent_Africa',\n",
    "                       'continent_Americas','continent_Asia',\n",
    "                       'continent_Europe','country',\n",
    "                       'pop','health_spend','logGDP','Pop density',\n",
    "                       'Democracy']]\n",
    "# X_opt.head(13)\n",
    "regressor_OLS = sm.OLS(endog = y_train, \n",
    "                      exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the table above.  We are particularly interested in the P-values for each variable.  What does this say about the significance of the different variables?  Are any of them surprising?\n",
    "\n",
    "#### You can plot the results using the user defined module make_plot, which sets some of the display parameters like colour and titles, without you having to type it all in every time.  The format is:\n",
    "make_plot('independent_variable',x_data,actual_data,predicted data,continents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "regressor_new = LinearRegression()\n",
    "regressor.fit(X_opt,y_train)\n",
    "y_pred_new = regressor.predict(X_opt)\n",
    "\n",
    "#plot the results\n",
    "gdp_per_cap = np.exp(X_train['logGDP'])\n",
    "make_plot('GDP per cap',gdp_per_cap,y_train,y_pred_new,cont_train)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_train['health_spend'],y_train,y_pred_new,cont_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now your turn!\n",
    "#### Use the cells below to drop some variables and make new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward elimination\n",
    "X_opt = X_train.loc[:,['constant_for_regression', \n",
    "                       'continent_Africa','continent_Americas',\n",
    "                       'continent_Asia','continent_Europe',\n",
    "#                       'country',\n",
    "                       'pop',\n",
    "                       'health_spend',\n",
    "                       'logGDP',\n",
    "                       'Pop density',\n",
    "                       'Democracy'\n",
    "                      ]]\n",
    "# X_opt.head(13)\n",
    "regressor_OLS = sm.OLS(endog = y_train, \n",
    "                      exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "regressor_new = LinearRegression()\n",
    "regressor_new.fit(X_opt,y_train)\n",
    "y_pred_new = regressor_new.predict(X_opt)\n",
    "\n",
    "#plot the results\n",
    "gdp_per_cap = np.exp(X_train['logGDP'])\n",
    "make_plot('GDP per cap',gdp_per_cap,y_train,y_pred_new,cont_train)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_train['health_spend'],y_train,y_pred_new,cont_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the same features as before\n",
    "X_opt_test = X_test.loc[:,['constant_for_regression', \n",
    "                       'continent_Africa','continent_Americas',\n",
    "                       'continent_Asia','continent_Europe',\n",
    "#                       'country',\n",
    "                       'pop',\n",
    "                       'health_spend',\n",
    "                       'logGDP',\n",
    "                       'Pop density',\n",
    "                       'Democracy'\n",
    "                      ]]\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "# regressor_test = LinearRegression()\n",
    "# regressor_test.fit(X_opt_test,y_train)\n",
    "y_pred_test = regressor_new.predict(X_opt_test)\n",
    "\n",
    "#plot the results\n",
    "gdp_per_cap = np.exp(X_test['logGDP'])\n",
    "make_plot('GDP per cap',gdp_per_cap,y_test,y_pred_test,cont_test)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_test['health_spend'],y_test,y_pred_test,cont_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
