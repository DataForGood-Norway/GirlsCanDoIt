{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Linear Regression using GapMinder Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a very simple example of machine learning we're going see if we can predict life expectancy using some data from [GapMinder](https://www.gapminder.org/tools/#_data_/_lastModified:1526038652718&lastModified:1526038652718;&chart-type=bubbles), an organisation that aims to educate us more on the true state of the world.\n",
    "The input data set will have the following variables:\n",
    "- Country - 136 countries are included \n",
    "- Continent\n",
    "- Life expectancy\n",
    "- GDP per capita, PPP, inflation adjusted\n",
    "- Healthcare spend as a percentage of GDP\n",
    "- Population per square km \n",
    "- The democracy index of the country (high is better). See https://en.wikipedia.org/wiki/Democracy_Index\n",
    "\n",
    "Go to [GapMinder](https://www.gapminder.org/tools/#_data_/_lastModified:1526038652718&lastModified:1526038652718;&chart-type=bubbles) now and take a look at some of these variable in the data viewer.  \n",
    "- Do any of the variables (visually) look like they have a bearing on life expectancy?  \n",
    "- Are any of them surprising?\n",
    "- We want the relationship between the variables to be more-or-less astraight line.  Does a log-linear display help? \n",
    "<img src=\"files/Screen Shot from gapminder.png\" alt=\"GDP per Capita vs. Life Expectancy from Gapminder\" title=\"GDP per Capita vs. Life Expectancy from Gapminder\" style=\"width: 50pc;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Linear Regression Theory\n",
    "- Straight line equation (indicate dependent vs. independent variable)\n",
    "- Least squares\n",
    "- Expand to multiple dimensions\n",
    "- Link to more detailed explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we know which variables to use?\n",
    "When we have a number of variables, it may not be immediately apparent which ones influence the final result.  We will usually find that we can and should  drop one or more independent variables because:\n",
    "- We don't want to include variables that aren't significant\n",
    "<img src=\"files/GarbageInGarbageOut.png\" alt=\"Garbage In - Garbage out\" title=\"Garbage In - Garbage Out\" style=\"width: 25pc;\"/>\n",
    "- When you are presenting your results you will have to explain why you included a variable; \"because I had the data\" is not a good enough reason!\n",
    "\n",
    "There are several ways to acheive but we will concentrate on \"backward elimination\".  To understand this you need to know about the \"P\" value.  A proper explanation is [here](https://www.mathbootcamps.com/what-is-a-p-value) but for now we'll just say that the lower the P value is, the more significant the variable.\n",
    "\n",
    "## Backward elimination\n",
    "1. Choose a maximum P value: 5% is a good value\n",
    "2. Run the linear regression using all the dependent variables. \n",
    "3. Look at the P values from the output, and choose the biggest.  \n",
    "4. If it is > than 5%, then drop that variable.\n",
    "5. Rerun the linear regression.\n",
    "6. Repeat steps 2-5 until all P-values are < 5%.  \n",
    "\n",
    "#### I will take you up to step 5, then the rest will be up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding\n",
    "## Importing the Dataset\n",
    "Firstly we need to import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard library for numerical analysis\n",
    "import numpy as np\n",
    "# Standard library for data manipulation\n",
    "import pandas as pd\n",
    "# User-defined library for making plots\n",
    "import sys\n",
    "sys.path.append(\"python\")\n",
    "from plot_functions import make_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import the dataset with data for 2007, which has the following columns:\n",
    "- Country - 136 countries are included\n",
    "- Continent\n",
    "- lifeexp - Life expectancy\n",
    "- gdpPercap - GDP per capita, PPP, inflation adjusted\n",
    "- health_spend - Healthcare spend as a percentage of GDP\n",
    "- Pop density - popultaion per square km\n",
    "- Democracy - The democracy index of the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://raw.githubusercontent.com/DataForGood-Norway/GirlsCanDoIt/master/MachineLearning/Lab/datasets/gapminder_2007_emma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is how the data looks:\n",
    "\n",
    "<img src=\"files/Screen Shot dataset.png\" alt=\"Data_table\" title=\"Data table\" style=\"width: 50pc;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Data preparation\n",
    "To get ready for the regression we need to make sure that all the columns contain numbers only:\n",
    "- Cells that are empty currently contain \"Not a Number\" (nan) - these will be replaced with an average\n",
    "- Cells that contain words (\"categorical variables\") will be replaced with numerical values.\n",
    "\n",
    "But firstly the data now needs to be split into independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country, continent, population, GDP per cap, healthcare spend, pop density, democracy\n",
    "X = dataset.iloc[:,[0,1,3,4,5,6,7]].values\n",
    "# Will use the log of the GDP per capita because from our first look, there seems to be a log-linear realtionship\n",
    "X[:,3] = np.log(dataset.iloc[:,4].values)\n",
    "# The dependent variable is life expectancy, in column 2\n",
    "y = dataset.iloc[:, 2].values\n",
    "# Also making a list of continents for plotting purposes\n",
    "cont = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "- The cells where data is missing have \"Not a Number\" in them. The following code replaces these with the average for that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the \"NaNs\" - replace with the average of that column\n",
    "# sklearn contains libraries for preprocessing data\n",
    "# now importing Imputer class\n",
    "from sklearn.preprocessing import Imputer\n",
    "# Create object\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "# Fit imputer object to feature X\n",
    "imputer = imputer.fit(X[:,2:])\n",
    "# Replace missing data with replaced values\n",
    "X[:,2:] = imputer.transform(X[:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "Categorical variables are ones which have a name, not a number.  In this case that would \"Country name\" and \"Continent\".\n",
    "\n",
    "__Country name__: We only have each country listed once, and it makes no sense to use the name of the country in the regression, so we are somply going to number them 0-136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now importing LabelEncoder class\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Encoding the Independent Variable\n",
    "# Firstly assign the categorical variables a unique number using LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "# Country names are in column zero\n",
    "# Each country will get a number 0-136 (for the full dataset) \n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Continent__: There are 5 continents in this dataset: Africa, Americas, Asia, Europe and Oceania.  If we simply give them a number 0-4, Python will assume that Oceania \"is greater\" than all this other continents, which makes no sense.  To fix this we use \"one-hot encoding\" which works like this:\n",
    "- Number the continents 0-4\n",
    "- Create a column for each continent -- these new columns are called __dummy variables__\n",
    "- If the country is in that continent the column contains \"1\" otherwise it contains \"0\"\n",
    "- Finally, since any of the five columns could be predicted from the other four (known as the __dummy variable trap__) we need to drop one.\n",
    "    - Normally it would be the first column, but since Africa looks quite interesting in this dataset, I decided to drop Oceania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now importing OneHotEncoder class\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Each continent will get a number 0-4\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "cont = labelencoder_X.fit_transform(cont)\n",
    "\n",
    "# one hot encode continenets\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# Remove a continent (dummy variable trap)\n",
    "# The obvious one to drop is Africa as it's the first column\n",
    "# However this is also the most interesting\n",
    "# so dropping Oceania instead\n",
    "X = X[:,[0,1,2,3,5,6,7,8,9,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add a column of ones at the beginning, as the regression requires a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to the begining.  \n",
    "# This is because the following procedure \n",
    "# will otherwise not have a constant in the regression model\n",
    "X = np.append(arr = np.ones((len(X),1)).astype(int), values = X, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into tesing and training set\n",
    "This is very important!  We will first try to create the model from the training set and then apply it to the test set to see if the model has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test, cont_train, cont_test = train_test_split(X, y, cont, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\"\"\"\n",
    "Columns are now\n",
    "0: constant, 1:is Africa 2: is Americas 3: is Asia 4:is Europe\n",
    "5: country 6:population 7:log(GDP per cap) \n",
    "8:health spend 9:pop density 10: dempocracy score\n",
    "\"\"\"\n",
    "X_train = pd.DataFrame(X_train,columns = ['const', 'africa','americas','asia','europe','country',\n",
    "                                          'population','health_spend','logGDP','pop_density','democracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the regressor\n",
    "This is where we get to the interesting part!\n",
    "\n",
    "Firstly we will train the regressor using all the variables and see how it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Start the training\n",
    "\"\"\"\n",
    "\n",
    "#train the regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "#Make predictions\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "#plot the results\n",
    "gdp_per_cap = np.exp(X_train[:,7])\n",
    "make_plot('GDP per cap',gdp_per_cap,y_train,y_pred,cont_train)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_train[:,8],y_train,y_pred,cont_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can we say about the above plots?  \n",
    "#### Is there a continent that stands out from the rest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start backward elimination\n",
    "As a reminder:\n",
    "1. Choose a maximum P value: 5% is a good value\n",
    "2. Run the linear regression using all the dependent variables.\n",
    "3. Look at the P values from the output, and choose the biggest.\n",
    "4. If it is > than 5%, then drop that variable.\n",
    "5. Rerun the linear regression.\n",
    "6. Repeat steps 2-5 until all P-values are < 5%.\n",
    "\n",
    "#### The code below is the first step and then you will have a chance to get involved and complete the other steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start backward elimination\n",
    "import statsmodels.formula.api as sm\n",
    "X_opt = X_train[ : , [0,1,2,3,4,5,6,7,8,9,10]]\n",
    "regressor_OLS = sm.OLS(endog = y_train, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the table above.  We are particularly interested in the P-values for each variable.  What does this say about the significance of the different variables?  Are any of them surprising?\n",
    "\n",
    "#### You can plot the results using the user defined module make_plot, which sets some of the display parameters like colour and titles, without you having to type it all in every time.  The format is:\n",
    "make_plot('independent_variable',x_data,actual_data,predicted data,continents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "regressor_new = LinearRegression()\n",
    "regressor_new.fit(X_opt,y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "#plot the results\n",
    "gdp_per_cap = np.exp(X_train[:,7])\n",
    "make_plot('GDP per cap',gdp_per_cap,y_train,y_pred,cont_train)\n",
    "#plot the results\n",
    "make_plot('Health spend',X_train[:,8],y_train,y_pred,cont_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
